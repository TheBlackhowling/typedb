# typedb Cursor Rules

This file contains rules and guidelines for AI assistants working on the typedb codebase.

## Test Modification Rules

### ⚠️ CRITICAL: Test Modification Policy

**DO NOT modify existing tests without explicit user consent - EVER.**

- **Absolute Rule**: No test modifications without explicit user approval, regardless of circumstances
- Existing tests are considered stable and should never be changed without user discussion

- **If a test is broken:**
  1. **DO NOT** fix it automatically
  2. **DO** discuss the issue with the user first
  3. **DO** explain what's broken and why
  4. **DO** propose a fix approach
  5. **DO** wait for explicit user consent before making any changes

- **If you think a test needs modification:**
  1. Ask the user for permission first
  2. Explain why the modification is necessary
  3. Propose the specific changes you want to make
  4. Wait for explicit approval before making changes

- **NO EXCEPTIONS**: Even fixing obvious typos or formatting issues requires user consent

### ⚠️ CRITICAL: Test Skipping Policy

**DO NOT skip tests without explicit user permission - EVER.**

- **Absolute Rule**: No test skipping (`t.Skip`, `t.Skipf`, `t.SkipNow`) without explicit user approval
- All tests should run unless there's a documented, user-approved reason to skip them

- **If you think a test should be skipped:**
  1. **DO NOT** add `t.Skip()` automatically
  2. **DO** discuss with the user first
  3. **DO** explain why the test should be skipped
  4. **DO** propose alternatives (e.g., conditional test, mock setup, etc.)
  5. **DO** wait for explicit user approval before adding skip logic
  6. **DO** document the reason for skipping in a comment if approved

- **Acceptable skip scenarios (with user approval):**
  - Missing test dependencies that cannot be mocked (e.g., database driver not available)
  - Platform-specific tests that don't apply to current environment
  - Tests that require external services that are unavailable
  - Tests that are intentionally disabled pending a fix

- **Unacceptable skip scenarios (fix instead of skip):**
  - Tests that fail due to bugs in the code
  - Tests that fail due to incorrect test setup
  - Tests that are flaky (fix the flakiness instead)
  - Tests that are slow (optimize instead of skip)

- **NO EXCEPTIONS**: Even skipping tests for missing dependencies requires user discussion and approval

## Test Addition Workflow

### ⚠️ MANDATORY: Test Requirements for All Changes

**Any code change MUST include comprehensive testing:**

1. **Unit Tests:**
   - Add unit tests for all new functionality
   - Use `sqlmock` for database mocking
   - Test all code paths and error cases
   - Ensure tests are isolated and don't depend on external state

2. **Integration Tests:**
   - Add integration tests for ALL supported database types:
     - PostgreSQL
     - MySQL
     - SQLite
     - Oracle
     - MSSQL
   - Use `integration_tests/*/` directory structure
   - Test with real database connections (via docker-compose)

3. **Test Coverage:**
   - New code must have ≥ 90% coverage
   - Overall coverage must remain ≥ 80%
   - Document coverage changes in PR

### Test Plan Document Generation

**When adding new tests, you MUST:**

1. **Before writing tests:**
   - Create a test plan document in `docs/backlog/` or appropriate location
   - Document what will be tested, why, and how
   - Include test cases, expected outcomes, and acceptance criteria
   - Specify which database types will be tested

2. **After tests are completed:**
   - Update the test plan document with:
     - Actual tests implemented
     - Coverage achieved
     - Test results for each database type
     - Any deviations from the original plan
     - Test results summary
   - Mark the plan as ready for review

3. **Before creating PR:**
   - Ensure test plan document is complete and accurate
   - Include link to test plan in PR description
   - Verify all database types have been tested

### Test Plan Document Format

Test plan documents should include:
- **Purpose**: Why these tests are needed
- **Scope**: What functionality is being tested
- **Test Cases**: List of test cases with descriptions
- **Implementation**: How tests will be implemented (sqlmock, integration, etc.)
- **Coverage Goals**: Expected coverage improvement
- **Acceptance Criteria**: What constitutes completion
- **Status**: Progress tracking (planned, in progress, completed, reviewed)

## Impact Analysis Requirements

### ⚠️ MANDATORY: Before Making Any Changes

**You MUST analyze the impact of your changes:**

1. **Impact on Existing Tests:**
   - Review all existing unit tests that might be affected
   - Review all existing integration tests that might be affected
   - Identify which tests may need updates (but DO NOT modify without user consent)
   - Document potential test impacts in your plan

2. **Impact on Examples:**
   - Review all examples in `examples/` directory
   - Identify examples that use changed APIs
   - If examples need updates due to API changes:
     - **DO NOT** update examples automatically
     - **DO** prompt the user to discuss example updates
     - **DO** explain what changed and why examples need updates
     - **DO** propose specific example changes for user review
     - **DO** wait for user approval before updating examples

3. **Impact on Documentation:**
   - Check if `API.md` needs updates
   - Check if `README.md` needs updates
   - Check if any other documentation needs updates

4. **Impact on Database Compatibility:**
   - Ensure changes work with all database types
   - Test database-specific behavior
   - Document any database-specific considerations

## Pre-PR Checklist

### ⚠️ MANDATORY: Before Creating Any PR

**You MUST complete these steps before creating a PR:**

1. **Run Unit Tests:**
   ```bash
   go test ./... -v
   ```
   - All unit tests must pass
   - No test failures allowed
   - All database types should be covered in unit tests

2. **Run Integration Tests:**
   ```bash
   # Use docker-compose for integration tests
   cd integration_tests
   docker-compose up -d
   go test ./... -v
   docker-compose down
   ```
   - All integration tests must pass
   - Verify ALL database types (PostgreSQL, MySQL, SQLite, Oracle, MSSQL)
   - Test with real database connections

3. **Verify Code Coverage:**
   ```bash
   go test -coverprofile=coverage.out -covermode=atomic ./...
   go tool cover -func coverage.out
   ```
   - Overall coverage must be ≥ 80%
   - New code coverage should be ≥ 90%
   - If coverage drops below 80%, add tests to bring it back up
   - Document coverage changes in PR

4. **Check for Linter Issues:**
   ```bash
   go vet ./...
   golangci-lint run
   ```
   - Fix any critical linter errors
   - Document any warnings that are acceptable

5. **Verify No Breaking Changes:**
   - Check that existing functionality still works
   - Review API changes if any
   - Verify examples still work (or discuss updates with user)

6. **Impact Analysis Summary:**
   - Document which tests were added/modified
   - Document which database types were tested
   - Document any example impacts (and user discussion if needed)
   - Document any documentation updates made

## Code Coverage Requirements

### Minimum Coverage Threshold

**Maintain code coverage above 80% at all times.**

- **Current Target**: ≥ 80% overall coverage
- **New Code**: Should aim for ≥ 90% coverage
- **Critical Functions**: Should have 100% coverage (e.g., security-sensitive code)

### Coverage Monitoring

- Run coverage before and after changes
- Document coverage changes in PR description
- If coverage drops below 80%, add tests before merging

### Coverage Exclusions

- Test files themselves don't need coverage
- Generated code doesn't need coverage
- Integration test helpers may have lower coverage requirements

## Change Impact Checklist

### Before Making Any Code Change

**You MUST consider:**

- [ ] How will this change affect existing unit tests?
- [ ] How will this change affect existing integration tests?
- [ ] Do I need to add unit tests for all database types?
- [ ] Do I need to add integration tests for all database types?
- [ ] Will this change break any examples?
- [ ] Do examples need updates? (Prompt user for discussion)
- [ ] Does API.md need updates?
- [ ] Does README.md need updates?
- [ ] Will this work with all database types?
- [ ] Is code coverage maintained ≥ 80%?

## General Guidelines

### Code Style

- Follow Go standard formatting (`go fmt`)
- Use meaningful variable and function names
- Add comments for complex logic
- Keep functions focused and under 1000 lines (project standard)

### Error Handling

- Always check and handle errors appropriately
- Use wrapped errors with context (`fmt.Errorf("...: %w", err)`)
- Provide clear, actionable error messages

### Documentation

- Update API.md when adding new public APIs
- Update README.md for user-facing changes
- Add inline comments for complex algorithms
- Document any non-obvious design decisions

### Database Portability

- Code should work with all supported databases (PostgreSQL, MySQL, SQLite, MSSQL, Oracle)
- **MANDATORY**: Test with ALL database types for any change
- Add both unit tests (sqlmock) and integration tests (real DB) for all database types
- Document database-specific behavior
- If a feature doesn't work with a database type, document it clearly

### Security

- Never log sensitive data (use `nolog` tag)
- Validate and sanitize user inputs
- Use parameterized queries (never string concatenation)
- Follow security best practices from security audit

## Workflow Guidelines

### ⚠️ MANDATORY: Starting a New Task

**Before starting ANY new task, you MUST:**

1. **Checkout main branch:**
   ```bash
   git checkout main
   ```

2. **Get latest changes:**
   ```bash
   git pull
   ```

3. **Create a new branch:**
   ```bash
   git checkout -b <branch-type>/<branch-name>
   ```

4. **Verify you're on the new branch:**
   ```bash
   git branch
   ```

**This ensures:**
- You have the latest codebase
- You're working from a clean state
- Your changes are isolated in a feature branch
- No conflicts with other work

**NO EXCEPTIONS**: Always start from main with latest changes before beginning any task.

### Branch Naming

- `test/` - For test additions/modifications
- `fix/` - For bug fixes
- `feat/` - For new features
- `refactor/` - For refactoring
- `docs/` - For documentation changes
- `chore/` - For maintenance tasks

### Static Analysis Requirements

**⚠️ MANDATORY: Run Static Analysis Before PR**

**You MUST run static analysis before creating any PR:**

1. **Run the static analysis script:**
   ```bash
   # Windows (PowerShell):
   .\scripts\static-analysis.ps1
   
   # Unix/Linux/Mac:
   ./scripts/static-analysis.sh
   ```

2. **All checks must pass:**
   - `go vet` - Built-in Go static analyzer (MUST pass, fieldalignment disabled as optimization suggestion)
   - `golangci-lint` - Comprehensive linter aggregator (errors MUST be fixed, warnings acceptable)
   - `staticcheck` - Advanced static analysis (MUST pass)
   - `gosec` - Security checker (security issues MUST be fixed)
   - `govulncheck` - Vulnerability checker (vulnerabilities MUST be fixed)
   - `errcheck` - Unchecked error checker (SHOULD be fixed unless intentional)

3. **Fix critical issues:**
   - Security vulnerabilities (gosec, govulncheck) - **MUST be fixed**
   - Errors from golangci-lint - **MUST be fixed**
   - Unchecked errors (errcheck) - **SHOULD be fixed** unless intentional

4. **Document intentional warnings:**
   - Style suggestions (field alignment, etc.) can be deferred
   - Document in PR description if warnings are intentional
   - Explain why they are acceptable or deferred

**NO EXCEPTIONS**: All static analysis checks must pass before creating a PR. The GitHub Actions workflow will also run these checks automatically on all PRs.

### Pre-PR Checklist

**Before creating ANY PR, you MUST verify:**

- [ ] Static analysis has been run and all checks pass (see Static Analysis Requirements above)
- [ ] All tests pass locally
- [ ] Code coverage is maintained ≥ 80%
- [ ] Impact analysis completed (see Change Impact Checklist)
- [ ] Documentation updated if needed (API.md, README.md)
- [ ] PR description follows the required format (see PR Creation section)

### Commit Messages

- Use conventional commit format: `type: description`
- Types: `test`, `fix`, `feat`, `refactor`, `docs`, `chore`
- Be descriptive but concise
- Reference issue numbers if applicable

### PR Creation

**⚠️ MANDATORY: PR Description Format**

When creating PRs, you MUST use the `/create-pr` command format with the exact metadata block structure:

```markdown
<!-- PR_METADATA_START -->
INTENTION: <Brief one-line description of the PR's purpose>
SUMMARY: <Detailed summary of changes, what was done, and why>
<!-- PR_METADATA_END -->
```

**Requirements:**
- The metadata block MUST be at the very beginning of the PR description
- `INTENTION:` must be a single line describing the PR's purpose
- `SUMMARY:` must be a detailed multi-line summary explaining:
  - What changes were made
  - Why the changes were made
  - What was tested
  - Any important notes or considerations
- After the metadata block, you can include additional details, sections, etc.
- Include test plan document link if adding tests
- Document coverage changes
- List all changes clearly
- Do NOT modify `versions/unreleased.md` or `CHANGELOG.md` (workflow handles this)

**Example:**
```markdown
<!-- PR_METADATA_START -->
INTENTION: Reduce cyclomatic complexity of deserializeToField function from 36 to below 15
SUMMARY: Extract type handlers into separate functions and refactor main function to use handler chain pattern. Complexity reduced from 36 to ~10. All existing tests pass with no functional changes.
<!-- PR_METADATA_END -->

## Changes

### Refactoring Strategy
...
```

## File Organization

### Test Files

- Unit tests: `*_test.go` in same package
- Integration tests: `integration_tests/*/`
- Test files should be under 1000 lines (split if needed)
- Group related tests in same file

### Documentation

- API documentation: `API.md` (root)
- Internal docs: `docs/` (not checked in)
- Backlog/plans: `docs/backlog/` (not checked in)
- Examples: `examples/`

## When in Doubt

- **Ask the user** before making significant changes
- **Preserve existing behavior** unless explicitly changing it
- **Add tests** for new functionality (unit + integration for all DB types)
- **Document** non-obvious decisions
- **Verify** changes work across all database types
- **Discuss** example updates with user if API changes
- **Never modify** tests without explicit user consent
- **Always consider** impact on existing tests and examples before making changes
- **Analyze impact** on unit tests, integration tests, examples, and documentation
